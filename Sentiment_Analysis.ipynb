{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "2dadf184-d95d-5d67-939e-a458ef65f877",
        "openai_ephemeral_user_id": "1976e6f0-353c-59a1-ae60-04d7ab77b7e7",
        "openai_subdivision1_iso_code": "US-TX"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "noteable": {
      "last_transaction_id": "87134213-cfc6-4339-a81b-df862e215cb9"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small"
  },
  "cells": [
    {
      "id": "707a0e9a-d2dd-453b-955f-995613dec6e1",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "",
      "outputs": []
    },
    {
      "id": "59fc0496-b196-4a25-8640-3d0d267ecbbe",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "ec4cb963-afcf-4ae0-ae48-c3a51bd794a0"
        },
        "ExecuteTime": {
          "end_time": "2023-07-27T20:30:55.643723+00:00",
          "start_time": "2023-07-27T20:30:52.485678+00:00"
        }
      },
      "execution_count": null,
      "source": "import nltk\nfrom nltk.corpus import movie_reviews\n\n# Download the movie reviews dataset\nnltk.download('movie_reviews')\n\n# Load the dataset\ndocuments = [(list(movie_reviews.words(fileid)), category)\n              for category in movie_reviews.categories()\n              for fileid in movie_reviews.fileids(category)]\n\n# Print the total number of documents\nprint('Total number of documents:', len(documents))\n\n# Print a sample document\nprint('\\nSample document:', documents[0])",
      "outputs": []
    },
    {
      "id": "66ee0bc2-cbd9-4d5c-88ca-63f46b66b19b",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "5ffea1ee-64b6-43f9-b34f-660d516c365d"
        },
        "ExecuteTime": {
          "end_time": "2023-07-27T20:31:00.565344+00:00",
          "start_time": "2023-07-27T20:30:55.651509+00:00"
        }
      },
      "execution_count": null,
      "source": "!pip install -q nltk",
      "outputs": []
    },
    {
      "id": "f35bce4a-9849-442d-b219-115e818c9d4c",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "abf0417c-bf32-4125-b7b3-a8522fbc2272"
        },
        "ExecuteTime": {
          "end_time": "2023-07-27T20:31:36.069271+00:00",
          "start_time": "2023-07-27T20:31:00.576779+00:00"
        }
      },
      "execution_count": null,
      "source": "from nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport string\n\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\n\n# Function to preprocess text\ndef preprocess_text(text):\n    # Tokenization\n    tokens = word_tokenize(text)\n\n    # Lowercase and remove punctuation\n    tokens = [token.lower() for token in tokens if token.isalpha()]\n\n    # Remove stopwords\n    tokens = [token for token in tokens if token not in stop_words]\n\n    # Lemmatization\n    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n\n    # Stemming\n    tokens = [stemmer.stem(token) for token in tokens]\n\n    return tokens\n\n# Preprocess all documents\ndocuments = [(preprocess_text(' '.join(doc)), category) for doc, category in documents]\n\n# Print a sample preprocessed document\nprint('Sample preprocessed document:', documents[0])",
      "outputs": []
    },
    {
      "id": "18eaa1a8-16bc-46cc-9f53-6e9ea2c99c44",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "ea4a4be2-f40a-49fb-bb54-b2c5ae5288dd"
        },
        "ExecuteTime": {
          "end_time": "2023-07-27T20:31:36.964149+00:00",
          "start_time": "2023-07-27T20:31:36.077893+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\n# Join the words back into one string separated by space,\n# and create a list of sentences.\nsentences = [' '.join(doc) for doc, category in documents]\ncategories = [category for doc, category in documents]\n\n# Create an instance of CountVectorizer\nvectorizer = CountVectorizer()\n\n# Convert sentences into vectors\nX = vectorizer.fit_transform(sentences)\n\n# Map categories to binary labels\ny = [1 if category == 'pos' else 0 for category in categories]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Print the shapes of the training and testing sets\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)\nprint('y_train shape:', len(y_train))\nprint('y_test shape:', len(y_test))",
      "outputs": []
    },
    {
      "id": "679847e3-5f96-4657-b4f8-0b3ae2327016",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "8526f37c-a32e-466d-8ec2-4403acc41f31"
        },
        "ExecuteTime": {
          "end_time": "2023-07-27T20:31:37.143987+00:00",
          "start_time": "2023-07-27T20:31:36.972174+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n\n# Create an instance of MultinomialNB\nclassifier = MultinomialNB()\n\n# Train the classifier\nclassifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = classifier.predict(X_test)\n\n# Compute the accuracy of the classifier\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy of the Naive Bayes classifier:', accuracy)",
      "outputs": []
    }
  ]
}